{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading required package: opendatatoronto\n",
      "\n",
      "\n",
      "Attaching package: 'dplyr'\n",
      "\n",
      "\n",
      "The following objects are masked from 'package:stats':\n",
      "\n",
      "    filter, lag\n",
      "\n",
      "\n",
      "The following objects are masked from 'package:base':\n",
      "\n",
      "    intersect, setdiff, setequal, union\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load all required libraries for the notebook, including data package\n",
    "if(!require(\"opendatatoronto\")) {\n",
    "    install.packages(\"opendatatoronto\")\n",
    "    library(opendatatoronto)\n",
    "}\n",
    "library(dplyr)\n",
    "library(stringr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Get Data - Traffice\n",
    "# output Data Description:\n",
    "# Dataframe with all intersection and daily count ( peak 4 hours), including lng / lat\n",
    "\n",
    "# ? and todo:\n",
    "# is separate street name needed\n",
    "# direction of the street to be determined. How?\n",
    "\n",
    "\n",
    "# package_traffic <- show_package(\"traffic-volumes-at-intersections-for-all-modes\")\n",
    "\n",
    "# get all resources for this package\n",
    "resources <- list_package_resources(\"traffic-volumes-at-intersections-for-all-modes\")\n",
    "\n",
    "# identify datastore resources; by default, Toronto Open Data sets datastore resource format to CSV for non-geospatial and GeoJSON for geospatial resources\n",
    "datastore_resources <- filter(resources, tolower(format) %in% c(\"csv\", \"geojson\"))\n",
    "\n",
    "# load data\n",
    "location <- filter(datastore_resources, row_number() == 1) %>% get_resource()\n",
    "traffic1 <- filter(datastore_resources, row_number() == 3) %>% get_resource()\n",
    "traffic2 <- filter(datastore_resources, row_number() == 4) %>% get_resource()\n",
    "traffic3 <- filter(datastore_resources, row_number() == 5) %>% get_resource()\n",
    "traffic4 <- filter(datastore_resources, row_number() == 6) %>% get_resource()\n",
    "traffic5 <- filter(datastore_resources, row_number() == 7) %>% get_resource()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[22m`summarise()` has grouped output by 'count_date', 'location_id', 'location',\n",
      "'lng', 'lat'. You can override using the `.groups` argument.\n",
      "\u001b[1m\u001b[22m`summarise()` has grouped output by 'count_date', 'location_id', 'location',\n",
      "'lng'. You can override using the `.groups` argument.\n",
      "\u001b[1m\u001b[22m`summarise()` has grouped output by 'count_date', 'location_id', 'location',\n",
      "'lng', 'lat'. You can override using the `.groups` argument.\n",
      "\u001b[1m\u001b[22m`summarise()` has grouped output by 'count_date', 'location_id', 'location',\n",
      "'lng'. You can override using the `.groups` argument.\n",
      "\u001b[1m\u001b[22m`summarise()` has grouped output by 'count_date', 'location_id', 'location',\n",
      "'lng', 'lat'. You can override using the `.groups` argument.\n",
      "\u001b[1m\u001b[22m`summarise()` has grouped output by 'count_date', 'location_id', 'location',\n",
      "'lng'. You can override using the `.groups` argument.\n",
      "\u001b[1m\u001b[22m`summarise()` has grouped output by 'count_date', 'location_id', 'location',\n",
      "'lng', 'lat'. You can override using the `.groups` argument.\n",
      "\u001b[1m\u001b[22m`summarise()` has grouped output by 'count_date', 'location_id', 'location',\n",
      "'lng'. You can override using the `.groups` argument.\n",
      "\u001b[1m\u001b[22m`summarise()` has grouped output by 'count_date', 'location_id', 'location',\n",
      "'lng', 'lat'. You can override using the `.groups` argument.\n",
      "\u001b[1m\u001b[22m`summarise()` has grouped output by 'count_date', 'location_id', 'location',\n",
      "'lng'. You can override using the `.groups` argument.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A grouped_df: 6 × 10</caption>\n",
       "<thead>\n",
       "\t<tr><th scope=col>count_date</th><th scope=col>location_id</th><th scope=col>location</th><th scope=col>lng</th><th scope=col>lat</th><th scope=col>total_int_traffic</th><th scope=col>nb_exit_traffic</th><th scope=col>sb_exit_traffic</th><th scope=col>wb_exit_traffic</th><th scope=col>eb_exit_traffic</th></tr>\n",
       "\t<tr><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;int&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><td>1984-01-03</td><td>4167</td><td>DUNDAS ST AT SPADINA AVE (PX 277)            </td><td>-79.39805</td><td>43.65294</td><td>8407</td><td>3286</td><td>2540</td><td>1545</td><td>1036</td></tr>\n",
       "\t<tr><td>1984-01-03</td><td>4386</td><td>DUNDAS ST AT SCARLETT RD (PX 496)            </td><td>-79.49921</td><td>43.66552</td><td>9414</td><td>1845</td><td>   0</td><td>4736</td><td>2833</td></tr>\n",
       "\t<tr><td>1984-01-03</td><td>5292</td><td>BAY ST AT DUNDAS ST W (PX 66)                </td><td>-79.38377</td><td>43.65571</td><td>8238</td><td>2545</td><td>2046</td><td>1643</td><td>2004</td></tr>\n",
       "\t<tr><td>1984-01-04</td><td>4993</td><td>EGLINTON AVE AT SPADINA RD (PX 99)           </td><td>-79.41911</td><td>43.70235</td><td>7868</td><td>   0</td><td>1539</td><td>4076</td><td>2253</td></tr>\n",
       "\t<tr><td>1984-01-04</td><td>5349</td><td>DUFFERIN ST AT DUNDAS ST W (PX 190)          </td><td>-79.43145</td><td>43.64965</td><td>6514</td><td>1685</td><td>1084</td><td>1820</td><td>1925</td></tr>\n",
       "\t<tr><td>1984-01-04</td><td>5589</td><td>DIXON AVE AT DUNDAS ST &amp; KINGSTON RD (PX 163)</td><td>-79.31118</td><td>43.66997</td><td>4625</td><td>2348</td><td>1075</td><td>1202</td><td><span style=white-space:pre-wrap>   0</span></td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A grouped\\_df: 6 × 10\n",
       "\\begin{tabular}{llllllllll}\n",
       " count\\_date & location\\_id & location & lng & lat & total\\_int\\_traffic & nb\\_exit\\_traffic & sb\\_exit\\_traffic & wb\\_exit\\_traffic & eb\\_exit\\_traffic\\\\\n",
       " <chr> & <int> & <chr> & <dbl> & <dbl> & <int> & <int> & <int> & <int> & <int>\\\\\n",
       "\\hline\n",
       "\t 1984-01-03 & 4167 & DUNDAS ST AT SPADINA AVE (PX 277)             & -79.39805 & 43.65294 & 8407 & 3286 & 2540 & 1545 & 1036\\\\\n",
       "\t 1984-01-03 & 4386 & DUNDAS ST AT SCARLETT RD (PX 496)             & -79.49921 & 43.66552 & 9414 & 1845 &    0 & 4736 & 2833\\\\\n",
       "\t 1984-01-03 & 5292 & BAY ST AT DUNDAS ST W (PX 66)                 & -79.38377 & 43.65571 & 8238 & 2545 & 2046 & 1643 & 2004\\\\\n",
       "\t 1984-01-04 & 4993 & EGLINTON AVE AT SPADINA RD (PX 99)            & -79.41911 & 43.70235 & 7868 &    0 & 1539 & 4076 & 2253\\\\\n",
       "\t 1984-01-04 & 5349 & DUFFERIN ST AT DUNDAS ST W (PX 190)           & -79.43145 & 43.64965 & 6514 & 1685 & 1084 & 1820 & 1925\\\\\n",
       "\t 1984-01-04 & 5589 & DIXON AVE AT DUNDAS ST \\& KINGSTON RD (PX 163) & -79.31118 & 43.66997 & 4625 & 2348 & 1075 & 1202 &    0\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A grouped_df: 6 × 10\n",
       "\n",
       "| count_date &lt;chr&gt; | location_id &lt;int&gt; | location &lt;chr&gt; | lng &lt;dbl&gt; | lat &lt;dbl&gt; | total_int_traffic &lt;int&gt; | nb_exit_traffic &lt;int&gt; | sb_exit_traffic &lt;int&gt; | wb_exit_traffic &lt;int&gt; | eb_exit_traffic &lt;int&gt; |\n",
       "|---|---|---|---|---|---|---|---|---|---|\n",
       "| 1984-01-03 | 4167 | DUNDAS ST AT SPADINA AVE (PX 277)             | -79.39805 | 43.65294 | 8407 | 3286 | 2540 | 1545 | 1036 |\n",
       "| 1984-01-03 | 4386 | DUNDAS ST AT SCARLETT RD (PX 496)             | -79.49921 | 43.66552 | 9414 | 1845 |    0 | 4736 | 2833 |\n",
       "| 1984-01-03 | 5292 | BAY ST AT DUNDAS ST W (PX 66)                 | -79.38377 | 43.65571 | 8238 | 2545 | 2046 | 1643 | 2004 |\n",
       "| 1984-01-04 | 4993 | EGLINTON AVE AT SPADINA RD (PX 99)            | -79.41911 | 43.70235 | 7868 |    0 | 1539 | 4076 | 2253 |\n",
       "| 1984-01-04 | 5349 | DUFFERIN ST AT DUNDAS ST W (PX 190)           | -79.43145 | 43.64965 | 6514 | 1685 | 1084 | 1820 | 1925 |\n",
       "| 1984-01-04 | 5589 | DIXON AVE AT DUNDAS ST &amp; KINGSTON RD (PX 163) | -79.31118 | 43.66997 | 4625 | 2348 | 1075 | 1202 |    0 |\n",
       "\n"
      ],
      "text/plain": [
       "  count_date location_id location                                     \n",
       "1 1984-01-03 4167        DUNDAS ST AT SPADINA AVE (PX 277)            \n",
       "2 1984-01-03 4386        DUNDAS ST AT SCARLETT RD (PX 496)            \n",
       "3 1984-01-03 5292        BAY ST AT DUNDAS ST W (PX 66)                \n",
       "4 1984-01-04 4993        EGLINTON AVE AT SPADINA RD (PX 99)           \n",
       "5 1984-01-04 5349        DUFFERIN ST AT DUNDAS ST W (PX 190)          \n",
       "6 1984-01-04 5589        DIXON AVE AT DUNDAS ST & KINGSTON RD (PX 163)\n",
       "  lng       lat      total_int_traffic nb_exit_traffic sb_exit_traffic\n",
       "1 -79.39805 43.65294 8407              3286            2540           \n",
       "2 -79.49921 43.66552 9414              1845               0           \n",
       "3 -79.38377 43.65571 8238              2545            2046           \n",
       "4 -79.41911 43.70235 7868                 0            1539           \n",
       "5 -79.43145 43.64965 6514              1685            1084           \n",
       "6 -79.31118 43.66997 4625              2348            1075           \n",
       "  wb_exit_traffic eb_exit_traffic\n",
       "1 1545            1036           \n",
       "2 4736            2833           \n",
       "3 1643            2004           \n",
       "4 4076            2253           \n",
       "5 1820            1925           \n",
       "6 1202               0           "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# clean and transform load - Traffic Data\n",
    "# Output data for modelling CleanTraffic\n",
    "# define parameters for cleaning\n",
    "peakhours <- 4 # number of peak hours of data per day. value should be between 1 and 10\n",
    "\n",
    "\n",
    "clean_T1 <- traffic1 %>%\n",
    "  select(one_of(c(\n",
    "    \"count_date\", \"location_id\", \"location\", \"lng\", \"lat\", \"centreline_type\",\n",
    "    \"time_start\", \"sb_cars_r\", \"sb_cars_t\", \"sb_cars_l\",\n",
    "    \"nb_cars_r\", \"nb_cars_t\", \"nb_cars_l\", \"wb_cars_r\", \"wb_cars_t\", \"wb_cars_l\",\n",
    "    \"eb_cars_r\", \"eb_cars_t\", \"eb_cars_l\"\n",
    "  ))) %>% # select needed attributes\n",
    "  filter(centreline_type == 2) %>% # only need intersection data\n",
    "  mutate(counthour = str_extract(time_start, \"(?<=T)(\\\\d+)(?=\\\\:)\")) %>% # extract hour\n",
    "  mutate(total_int_traffic = sb_cars_r + sb_cars_t + sb_cars_l +\n",
    "    nb_cars_r + nb_cars_t + nb_cars_l + wb_cars_r + wb_cars_t +\n",
    "    wb_cars_l + eb_cars_r + eb_cars_t + eb_cars_l) %>% # get total sum\n",
    "  mutate(nb_exit_traffic = nb_cars_t + eb_cars_l + wb_cars_r) %>% # get north bound exit volume\n",
    "  mutate(sb_exit_traffic = sb_cars_t + eb_cars_r + wb_cars_l) %>% # get south bound exit volume\n",
    "  mutate(wb_exit_traffic = wb_cars_t + nb_cars_l + sb_cars_r) %>% # get west bound exit volume\n",
    "  mutate(eb_exit_traffic = eb_cars_t + nb_cars_r + sb_cars_l) %>% # get east bound exit volume\n",
    "  select(one_of(c(\n",
    "    \"count_date\", \"location_id\", \"location\", \"lng\", \"lat\", \"counthour\",\n",
    "    \"total_int_traffic\", \"nb_exit_traffic\", \"sb_exit_traffic\", \"wb_exit_traffic\",\n",
    "    \"eb_exit_traffic\"\n",
    "  ))) %>% # remove raw attributes, retain aggregate only\n",
    "  group_by(across(all_of(c(\"count_date\", \"location_id\", \"location\", \"lng\", \"lat\", \"counthour\")))) %>%\n",
    "  summarise(across(any_of(c(\n",
    "    \"total_int_traffic\", \"nb_exit_traffic\", \"sb_exit_traffic\", \"wb_exit_traffic\",\n",
    "    \"eb_exit_traffic\"\n",
    "  )), sum)) %>% # agregate hourly volume\n",
    "  group_by(across(all_of(c(\"count_date\", \"location_id\", \"location\", \"lng\", \"lat\")))) %>%\n",
    "  slice_max(order_by = total_int_traffic, n = peakhours) %>% # filter top peak hour volume\n",
    "  group_by(across(all_of(c(\"count_date\", \"location_id\", \"location\", \"lng\", \"lat\")))) %>%\n",
    "  summarise(across(any_of(c(\n",
    "    \"total_int_traffic\", \"nb_exit_traffic\", \"sb_exit_traffic\", \"wb_exit_traffic\",\n",
    "    \"eb_exit_traffic\"\n",
    "  )), sum)) # aggregate daily peak hour volume\n",
    "clean_T2 <- traffic2 %>%\n",
    "  select(one_of(c(\n",
    "    \"count_date\", \"location_id\", \"location\", \"lng\", \"lat\", \"centreline_type\",\n",
    "    \"time_start\", \"sb_cars_r\", \"sb_cars_t\", \"sb_cars_l\",\n",
    "    \"nb_cars_r\", \"nb_cars_t\", \"nb_cars_l\", \"wb_cars_r\", \"wb_cars_t\", \"wb_cars_l\",\n",
    "    \"eb_cars_r\", \"eb_cars_t\", \"eb_cars_l\"\n",
    "  ))) %>% # select needed attributes\n",
    "  filter(centreline_type == 2) %>% # only need intersection data\n",
    "  mutate(counthour = str_extract(time_start, \"(?<=T)(\\\\d+)(?=\\\\:)\")) %>% # extract hour\n",
    "  mutate(total_int_traffic = sb_cars_r + sb_cars_t + sb_cars_l +\n",
    "    nb_cars_r + nb_cars_t + nb_cars_l + wb_cars_r + wb_cars_t +\n",
    "    wb_cars_l + eb_cars_r + eb_cars_t + eb_cars_l) %>% # get total sum\n",
    "  mutate(nb_exit_traffic = nb_cars_t + eb_cars_l + wb_cars_r) %>% # get north bound exit volume\n",
    "  mutate(sb_exit_traffic = sb_cars_t + eb_cars_r + wb_cars_l) %>% # get south bound exit volume\n",
    "  mutate(wb_exit_traffic = wb_cars_t + nb_cars_l + sb_cars_r) %>% # get west bound exit volume\n",
    "  mutate(eb_exit_traffic = eb_cars_t + nb_cars_r + sb_cars_l) %>% # get east bound exit volume\n",
    "  select(one_of(c(\n",
    "    \"count_date\", \"location_id\", \"location\", \"lng\", \"lat\", \"counthour\",\n",
    "    \"total_int_traffic\", \"nb_exit_traffic\", \"sb_exit_traffic\", \"wb_exit_traffic\",\n",
    "    \"eb_exit_traffic\"\n",
    "  ))) %>% # remove raw attributes, retain aggsregate only\n",
    "  group_by(across(all_of(c(\"count_date\", \"location_id\", \"location\", \"lng\", \"lat\", \"counthour\")))) %>%\n",
    "  summarise(across(any_of(c(\n",
    "    \"total_int_traffic\", \"nb_exit_traffic\", \"sb_exit_traffic\", \"wb_exit_traffic\",\n",
    "    \"eb_exit_traffic\"\n",
    "  )), sum)) %>% # agregate hourly volume\n",
    "  group_by(across(all_of(c(\"count_date\", \"location_id\", \"location\", \"lng\", \"lat\")))) %>%\n",
    "  slice_max(order_by = total_int_traffic, n = peakhours) %>% # filter top peak hour volume\n",
    "  group_by(across(all_of(c(\"count_date\", \"location_id\", \"location\", \"lng\", \"lat\")))) %>%\n",
    "  summarise(across(any_of(c(\n",
    "    \"total_int_traffic\", \"nb_exit_traffic\", \"sb_exit_traffic\", \"wb_exit_traffic\",\n",
    "    \"eb_exit_traffic\"\n",
    "  )), sum)) # aggregate daily peak hour volume\n",
    "\n",
    "clean_T3 <- traffic3 %>%\n",
    "  select(one_of(c(\n",
    "    \"count_date\", \"location_id\", \"location\", \"lng\", \"lat\", \"centreline_type\",\n",
    "    \"time_start\", \"sb_cars_r\", \"sb_cars_t\", \"sb_cars_l\",\n",
    "    \"nb_cars_r\", \"nb_cars_t\", \"nb_cars_l\", \"wb_cars_r\", \"wb_cars_t\", \"wb_cars_l\",\n",
    "    \"eb_cars_r\", \"eb_cars_t\", \"eb_cars_l\"\n",
    "  ))) %>% # select needed attributes\n",
    "  filter(centreline_type == 2) %>% # only need intersection data\n",
    "  mutate(counthour = str_extract(time_start, \"(?<=T)(\\\\d+)(?=\\\\:)\")) %>% # extract hour\n",
    "  mutate(total_int_traffic = sb_cars_r + sb_cars_t + sb_cars_l +\n",
    "    nb_cars_r + nb_cars_t + nb_cars_l + wb_cars_r + wb_cars_t +\n",
    "    wb_cars_l + eb_cars_r + eb_cars_t + eb_cars_l) %>% # get total sum\n",
    "  mutate(nb_exit_traffic = nb_cars_t + eb_cars_l + wb_cars_r) %>% # get north bound exit volume\n",
    "  mutate(sb_exit_traffic = sb_cars_t + eb_cars_r + wb_cars_l) %>% # get south bound exit volume\n",
    "  mutate(wb_exit_traffic = wb_cars_t + nb_cars_l + sb_cars_r) %>% # get west bound exit volume\n",
    "  mutate(eb_exit_traffic = eb_cars_t + nb_cars_r + sb_cars_l) %>% # get east bound exit volume\n",
    "  select(one_of(c(\n",
    "    \"count_date\", \"location_id\", \"location\", \"lng\", \"lat\", \"counthour\",\n",
    "    \"total_int_traffic\", \"nb_exit_traffic\", \"sb_exit_traffic\", \"wb_exit_traffic\",\n",
    "    \"eb_exit_traffic\"\n",
    "  ))) %>% # remove raw attributes, retain aggregate only\n",
    "  group_by(across(all_of(c(\"count_date\", \"location_id\", \"location\", \"lng\", \"lat\", \"counthour\")))) %>%\n",
    "  summarise(across(any_of(c(\n",
    "    \"total_int_traffic\", \"nb_exit_traffic\", \"sb_exit_traffic\", \"wb_exit_traffic\",\n",
    "    \"eb_exit_traffic\"\n",
    "  )), sum)) %>% # agregate hourly volume\n",
    "  group_by(across(all_of(c(\"count_date\", \"location_id\", \"location\", \"lng\", \"lat\")))) %>%\n",
    "  slice_max(order_by = total_int_traffic, n = peakhours) %>% # filter top peak hour volume\n",
    "  group_by(across(all_of(c(\"count_date\", \"location_id\", \"location\", \"lng\", \"lat\")))) %>%\n",
    "  summarise(across(any_of(c(\n",
    "    \"total_int_traffic\", \"nb_exit_traffic\", \"sb_exit_traffic\", \"wb_exit_traffic\",\n",
    "    \"eb_exit_traffic\"\n",
    "  )), sum)) # aggregate daily peak hour volume\n",
    "\n",
    "clean_T4 <- traffic4 %>%\n",
    "  select(one_of(c(\n",
    "    \"count_date\", \"location_id\", \"location\", \"lng\", \"lat\", \"centreline_type\",\n",
    "    \"time_start\", \"sb_cars_r\", \"sb_cars_t\", \"sb_cars_l\",\n",
    "    \"nb_cars_r\", \"nb_cars_t\", \"nb_cars_l\", \"wb_cars_r\", \"wb_cars_t\", \"wb_cars_l\",\n",
    "    \"eb_cars_r\", \"eb_cars_t\", \"eb_cars_l\"\n",
    "  ))) %>% # select needed attributes\n",
    "  filter(centreline_type == 2) %>% # only need intersection data\n",
    "  mutate(counthour = str_extract(time_start, \"(?<=T)(\\\\d+)(?=\\\\:)\")) %>% # extract hour\n",
    "  mutate(total_int_traffic = sb_cars_r + sb_cars_t + sb_cars_l +\n",
    "    nb_cars_r + nb_cars_t + nb_cars_l + wb_cars_r + wb_cars_t +\n",
    "    wb_cars_l + eb_cars_r + eb_cars_t + eb_cars_l) %>% # get total sum\n",
    "  mutate(nb_exit_traffic = nb_cars_t + eb_cars_l + wb_cars_r) %>% # get north bound exit volume\n",
    "  mutate(sb_exit_traffic = sb_cars_t + eb_cars_r + wb_cars_l) %>% # get south bound exit volume\n",
    "  mutate(wb_exit_traffic = wb_cars_t + nb_cars_l + sb_cars_r) %>% # get west bound exit volume\n",
    "  mutate(eb_exit_traffic = eb_cars_t + nb_cars_r + sb_cars_l) %>% # get east bound exit volume\n",
    "  select(one_of(c(\n",
    "    \"count_date\", \"location_id\", \"location\", \"lng\", \"lat\", \"counthour\",\n",
    "    \"total_int_traffic\", \"nb_exit_traffic\", \"sb_exit_traffic\", \"wb_exit_traffic\",\n",
    "    \"eb_exit_traffic\"\n",
    "  ))) %>% # remove raw attributes, retain aggregate only\n",
    "  group_by(across(all_of(c(\"count_date\", \"location_id\", \"location\", \"lng\", \"lat\", \"counthour\")))) %>%\n",
    "  summarise(across(any_of(c(\n",
    "    \"total_int_traffic\", \"nb_exit_traffic\", \"sb_exit_traffic\", \"wb_exit_traffic\",\n",
    "    \"eb_exit_traffic\"\n",
    "  )), sum)) %>% # agregate hourly volume\n",
    "  group_by(across(all_of(c(\"count_date\", \"location_id\", \"location\", \"lng\", \"lat\")))) %>%\n",
    "  slice_max(order_by = total_int_traffic, n = peakhours) %>% # filter top peak hour volume\n",
    "  group_by(across(all_of(c(\"count_date\", \"location_id\", \"location\", \"lng\", \"lat\")))) %>%\n",
    "  summarise(across(any_of(c(\n",
    "    \"total_int_traffic\", \"nb_exit_traffic\", \"sb_exit_traffic\", \"wb_exit_traffic\",\n",
    "    \"eb_exit_traffic\"\n",
    "  )), sum)) # aggregate daily peak hour volume\n",
    "\n",
    "clean_T5 <- traffic5 %>%\n",
    "  select(one_of(c(\n",
    "    \"count_date\", \"location_id\", \"location\", \"lng\", \"lat\", \"centreline_type\",\n",
    "    \"time_start\", \"sb_cars_r\", \"sb_cars_t\", \"sb_cars_l\",\n",
    "    \"nb_cars_r\", \"nb_cars_t\", \"nb_cars_l\", \"wb_cars_r\", \"wb_cars_t\", \"wb_cars_l\",\n",
    "    \"eb_cars_r\", \"eb_cars_t\", \"eb_cars_l\"\n",
    "  ))) %>% # select needed attributes\n",
    "  filter(centreline_type == 2) %>% # only need intersection data\n",
    "  mutate(counthour = str_extract(time_start, \"(?<=T)(\\\\d+)(?=\\\\:)\")) %>% # extract hour\n",
    "  mutate(total_int_traffic = sb_cars_r + sb_cars_t + sb_cars_l +\n",
    "    nb_cars_r + nb_cars_t + nb_cars_l + wb_cars_r + wb_cars_t +\n",
    "    wb_cars_l + eb_cars_r + eb_cars_t + eb_cars_l) %>% # get total sum\n",
    "  mutate(nb_exit_traffic = nb_cars_t + eb_cars_l + wb_cars_r) %>% # get north bound exit volume\n",
    "  mutate(sb_exit_traffic = sb_cars_t + eb_cars_r + wb_cars_l) %>% # get south bound exit volume\n",
    "  mutate(wb_exit_traffic = wb_cars_t + nb_cars_l + sb_cars_r) %>% # get west bound exit volume\n",
    "  mutate(eb_exit_traffic = eb_cars_t + nb_cars_r + sb_cars_l) %>% # get east bound exit volume\n",
    "  select(one_of(c(\n",
    "    \"count_date\", \"location_id\", \"location\", \"lng\", \"lat\", \"counthour\",\n",
    "    \"total_int_traffic\", \"nb_exit_traffic\", \"sb_exit_traffic\", \"wb_exit_traffic\",\n",
    "    \"eb_exit_traffic\"\n",
    "  ))) %>% # remove raw attributes, retain aggregate only\n",
    "  group_by(across(all_of(c(\"count_date\", \"location_id\", \"location\", \"lng\", \"lat\", \"counthour\")))) %>%\n",
    "  summarise(across(any_of(c(\n",
    "    \"total_int_traffic\", \"nb_exit_traffic\", \"sb_exit_traffic\", \"wb_exit_traffic\",\n",
    "    \"eb_exit_traffic\"\n",
    "  )), sum)) %>% # agregate hourly volume\n",
    "  group_by(across(all_of(c(\"count_date\", \"location_id\", \"location\", \"lng\", \"lat\")))) %>%\n",
    "  slice_max(order_by = total_int_traffic, n = peakhours) %>% # filter top peak hour volume\n",
    "  group_by(across(all_of(c(\"count_date\", \"location_id\", \"location\", \"lng\", \"lat\")))) %>%\n",
    "  summarise(across(any_of(c(\n",
    "    \"total_int_traffic\", \"nb_exit_traffic\", \"sb_exit_traffic\", \"wb_exit_traffic\",\n",
    "    \"eb_exit_traffic\"\n",
    "  )), sum)) # aggregate daily peak hour volume\n",
    "\n",
    "\n",
    "CleanTraffic <- bind_rows(clean_T1, clean_T2, clean_T3, clean_T4, clean_T5)\n",
    "head(CleanTraffic)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Attaching package: 'tidyjson'\n",
      "\n",
      "\n",
      "The following object is masked from 'package:stats':\n",
      "\n",
      "    filter\n",
      "\n",
      "\n",
      "── \u001b[1mAttaching core tidyverse packages\u001b[22m ──────────────────────── tidyverse 2.0.0 ──\n",
      "\u001b[32m✔\u001b[39m \u001b[34mforcats  \u001b[39m 1.0.0     \u001b[32m✔\u001b[39m \u001b[34mreadr    \u001b[39m 2.1.4\n",
      "\u001b[32m✔\u001b[39m \u001b[34mlubridate\u001b[39m 1.9.2     \u001b[32m✔\u001b[39m \u001b[34mtibble   \u001b[39m 3.2.1\n",
      "\u001b[32m✔\u001b[39m \u001b[34mpurrr    \u001b[39m 1.0.2     \u001b[32m✔\u001b[39m \u001b[34mtidyr    \u001b[39m 1.3.0\n",
      "── \u001b[1mConflicts\u001b[22m ────────────────────────────────────────── tidyverse_conflicts() ──\n",
      "\u001b[31m✖\u001b[39m \u001b[34mtidyjson\u001b[39m::\u001b[32mfilter()\u001b[39m masks \u001b[34mdplyr\u001b[39m::filter(), \u001b[34mstats\u001b[39m::filter()\n",
      "\u001b[31m✖\u001b[39m \u001b[34mdplyr\u001b[39m::\u001b[32mlag()\u001b[39m       masks \u001b[34mstats\u001b[39m::lag()\n",
      "\u001b[36mℹ\u001b[39m Use the conflicted package (\u001b[3m\u001b[34m<http://conflicted.r-lib.org/>\u001b[39m\u001b[23m) to force all conflicts to become errors\n",
      "Linking to GEOS 3.11.2, GDAL 3.8.2, PROJ 9.3.1; sf_use_s2() is TRUE\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".dl-inline {width: auto; margin:0; padding: 0}\n",
       ".dl-inline>dt, .dl-inline>dd {float: none; width: auto; display: inline-block}\n",
       ".dl-inline>dt::after {content: \":\\0020\"; padding-right: .5ex}\n",
       ".dl-inline>dt:not(:first-of-type) {padding-left: .5ex}\n",
       "</style><dl class=dl-inline><dt>address</dt><dd>'character'</dd><dt>lat</dt><dd>'character'</dd><dt>lng</dt><dd>'character'</dd><dt>carpark_type</dt><dd>'character'</dd><dt>rate_half_hr</dt><dd>'character'</dd><dt>capacity</dt><dd>'character'</dd></dl>\n"
      ],
      "text/latex": [
       "\\begin{description*}\n",
       "\\item[address] 'character'\n",
       "\\item[lat] 'character'\n",
       "\\item[lng] 'character'\n",
       "\\item[carpark\\textbackslash{}\\_type] 'character'\n",
       "\\item[rate\\textbackslash{}\\_half\\textbackslash{}\\_hr] 'character'\n",
       "\\item[capacity] 'character'\n",
       "\\end{description*}\n"
      ],
      "text/markdown": [
       "address\n",
       ":   'character'lat\n",
       ":   'character'lng\n",
       ":   'character'carpark_type\n",
       ":   'character'rate_half_hr\n",
       ":   'character'capacity\n",
       ":   'character'\n",
       "\n"
      ],
      "text/plain": [
       "     address          lat          lng carpark_type rate_half_hr     capacity \n",
       " \"character\"  \"character\"  \"character\"  \"character\"  \"character\"  \"character\" "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".dl-inline {width: auto; margin:0; padding: 0}\n",
       ".dl-inline>dt, .dl-inline>dd {float: none; width: auto; display: inline-block}\n",
       ".dl-inline>dt::after {content: \":\\0020\"; padding-right: .5ex}\n",
       ".dl-inline>dt:not(:first-of-type) {padding-left: .5ex}\n",
       "</style><dl class=dl-inline><dt>address</dt><dd>'character'</dd><dt>lat</dt><dd>'numeric'</dd><dt>lng</dt><dd>'numeric'</dd><dt>carpark_type</dt><dd>'character'</dd><dt>rate_half_hr</dt><dd>'numeric'</dd><dt>capacity</dt><dd>'numeric'</dd></dl>\n"
      ],
      "text/latex": [
       "\\begin{description*}\n",
       "\\item[address] 'character'\n",
       "\\item[lat] 'numeric'\n",
       "\\item[lng] 'numeric'\n",
       "\\item[carpark\\textbackslash{}\\_type] 'character'\n",
       "\\item[rate\\textbackslash{}\\_half\\textbackslash{}\\_hr] 'numeric'\n",
       "\\item[capacity] 'numeric'\n",
       "\\end{description*}\n"
      ],
      "text/markdown": [
       "address\n",
       ":   'character'lat\n",
       ":   'numeric'lng\n",
       ":   'numeric'carpark_type\n",
       ":   'character'rate_half_hr\n",
       ":   'numeric'capacity\n",
       ":   'numeric'\n",
       "\n"
      ],
      "text/plain": [
       "     address          lat          lng carpark_type rate_half_hr     capacity \n",
       " \"character\"    \"numeric\"    \"numeric\"  \"character\"    \"numeric\"    \"numeric\" "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "FALSE"
      ],
      "text/latex": [
       "FALSE"
      ],
      "text/markdown": [
       "FALSE"
      ],
      "text/plain": [
       "[1] FALSE"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Passing 220 coordinates to the Nominatim single coordinate geocoder\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# install.packages(\"tidyjson\") \n",
    "# install.packages(\"tidygeocoder\")\n",
    "# install.packages(\"sf\")\n",
    "# install.packages(\"mapview\")\n",
    "# install.packages(\"ggspatial\")\n",
    "\n",
    "library(ggplot2)\n",
    "library(ggspatial)\n",
    "library(tidyjson)\n",
    "library(dplyr)\n",
    "library(tidyverse)\n",
    "library(tidygeocoder)\n",
    "library(sf)\n",
    "library(mapview)\n",
    "library(stringr)\n",
    "library(opendatatoronto)\n",
    "\n",
    "\n",
    "# Get Green P Parking package from Open Data-Toronto\n",
    "package <- show_package(\"b66466c3-69c8-4825-9c8b-04b270069193\")\n",
    "\n",
    "data=as.data.frame('Green P Parking')  # read the dataset Green P Parking in the package \n",
    "data<-show_package(package)\n",
    "\n",
    "resources<-list_package_resources(package)\n",
    "\n",
    "# Identify resources\n",
    "data_resources <- filter(resources, tolower(format) %in% c(\"csv\", \"json\"))\n",
    "\n",
    "# Load Green P Parking 2019 data\n",
    "data <- filter(data_resources, row_number() == 1) %>% get_resource()\n",
    "\n",
    "# Extract required columns from main data\n",
    "data<- data.frame(address=data$carparks$address,\n",
    "                                     lat=data$carparks$lat,\n",
    "                                     lng=data$carparks$lng,\n",
    "                                      carpark_type=data$carparks$carpark_type_str,\n",
    "                                      rate_half_hr=data$carparks$rate_half_hour,\n",
    "                                      capacity=data$carparks$capacity\n",
    "                  )\n",
    "\n",
    "# Check class of each attribute\n",
    "sapply(data, class) \n",
    "\n",
    "# Convert char to numeric class\n",
    "data$lat<-as.numeric(data$lat)\n",
    "data$lng<-as.numeric(data$lng)\n",
    "data$rate_half_hr<-as.numeric(data$rate_half_hr)\n",
    "data$capacity<-as.numeric(data$capacity)\n",
    "\n",
    "sapply(data, class) \n",
    "\n",
    "# Check for missing values\n",
    "any(is.na(data))\n",
    "\n",
    "# Extract street name from address\n",
    "data <- data %>%\n",
    "  mutate(extracted_address = str_replace_all(data$address, \"\\\\(.*?\\\\)\",\"\"))\n",
    "\n",
    "data$extracted_address<-str_replace_all(data$extracted_address, \"-.*\", \"\")\n",
    "data$extracted_address<-str_replace_all(data$extracted_address, \",.*\", \"\")\n",
    "\n",
    "# Extract data with carpark_type as 'Surface'\n",
    "data <- data %>%\n",
    "  filter(carpark_type == \"Surface\")\n",
    "\n",
    "# Convert Lat/Lng to address\n",
    "geo_rev_data<-data %>%\n",
    "  tidygeocoder::reverse_geocode(\n",
    "    lat=lat,\n",
    "    long=lng,\n",
    "    method=\"osm\")\n",
    "\n",
    "# Plot map\n",
    "map<-data %>%\n",
    "  st_as_sf(\n",
    "    coords=c(\"lng\",\"lat\"),\n",
    "    crs=4326\n",
    "  )\n",
    "\n",
    "map %>% mapview()\n",
    "\n",
    "# # Filter data to extract only parking spots with M5C pin code\n",
    "# filtered_df <- geo_rev_data %>%\n",
    "#   filter(grepl(\"M5C\", address...8))\n",
    "# \n",
    "# # Print the filtered data frame\n",
    "# print(filtered_df)\n",
    "# \n",
    "# # Convert Lat/Lng to address and plot map\n",
    "# filtered_df_map<-filtered_df %>%\n",
    "#   st_as_sf(\n",
    "#     coords=c(\"lng\",\"lat\"),\n",
    "#     crs=4326\n",
    "#   )\n",
    "# \n",
    "# filtered_df_map %>% mapview()\n",
    "\n",
    "# # Extract street names\n",
    "# \n",
    "# filtered_df_map$streets <- gsub(\"\\\\d+\", \"\", filtered_df_map$extracted_address) # Remove numbers # nolint # nolint\n",
    "# filtered_df_map$streets <- gsub(\"\\\\.$\", \"\", filtered_df_map$streets) # Remove trailing periods # nolint\n",
    "# \n",
    "# filtered_df_map$lat <- filtered_df$lat\n",
    "# \n",
    "# filtered_df_map$lng <- filtered_df$lng\n",
    "\n",
    "\n",
    "# Define the polygon coordinates\n",
    "polygon_coords <- matrix(c(\n",
    "  -79.4289964889767, 43.6700360241176, -79.4226792245877, 43.6543887000655, -79.4000484228339, 43.657948514946, -79.4070875693025, 43.6748646586934,-79.4289964889767, 43.6700360241176  # Repeat first point to close the polygon exactly\n",
    "), ncol = 2, byrow = TRUE)\n",
    "\n",
    "# Create a polygon geometry\n",
    "polygon <- st_polygon(list(polygon_coords))\n",
    "\n",
    "# Convert the polygon object to an 'sf' object with a specified CRS (Coordinate Reference System)\n",
    "polygon <- st_sfc(polygon, crs = 4326)\n",
    "\n",
    "# Convert the data frame 'data' to an 'sf' object, specifying the columns containing longitude and latitude as coordinates, and set the CRS\n",
    "data_sf <- st_as_sf(data, coords = c(\"lng\", \"lat\"), crs = 4326)\n",
    "\n",
    "# Find the indices of the rows in 'data_sf' that fall within the polygon\n",
    "indices <- st_within(data_sf, polygon)\n",
    "\n",
    "# Convert the indices to a data frame\n",
    "indices_df<-data.frame(indices)\n",
    "\n",
    "selected_rows <- data_sf[match(indices_df$row.id, seq_len(nrow(data))), ]\n",
    "\n",
    "# Print the selected rows\n",
    "print(selected_rows) \n",
    "\n",
    "# Plot selected rows on map\n",
    "map_df <- selected_rows %>%\n",
    "  st_as_sf() %>%\n",
    "  st_set_crs(4326) %>%\n",
    "  fortify()   # Convert the 'sf' object to a format suitable for use with 'ggplot2'\n",
    "\n",
    "# Display the spatial object using 'mapview'\n",
    "mapview(map_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Get Data - Intersection\n",
    "# output Data Description:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Get Data - Business\n",
    "# output Data Description: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Define Region of Interest - Boundary\n",
    "## coordinates manually looked up from location dataset\n",
    "# 1406\t5370\tDUPONT ST AT OSSINGTON AVE (PX 842)\t-79.429019\t43.670031996501194\n",
    "# 251\t4180\tDUPONT ST AT SPADINA RD (PX 840)\t-79.407122\t43.67485699954096\n",
    "# 1885\t5864\tCOLLEGE ST AT OSSINGTON AVE (PX 829)\t-79.422705\t43.65439999619167\n",
    "# 241\t4170\tCOLLEGE ST AT SPADINA AVE (PX 279)\t-79.400048\t43.65794800150128\n",
    "\n",
    "# Input\n",
    "# Output\n",
    "\n",
    "boundary <- location %>%\n",
    "  select(location_id, location, lng, lat) %>%\n",
    "  filter(location_id %in% list(5370, 4180, 5864, 4170)) # boundary intersection ID\n",
    "\n",
    "lng_min <- min(boundary$lng) # west most value since it's negative\n",
    "lng_max <- max(boundary$lng) # east most value\n",
    "lat_min <- min(boundary$lat) # south most value\n",
    "lat_max <- max(boundary$lat) # north most value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Combine Data for model building\n",
    "# what is the expected output of data structure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Model 1 - Time Series Forecast\n",
    "# additional data processing needed before modelling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Model 2 - Regression Model\n",
    "\n",
    "# Load required libraries\n",
    "library(sf)\n",
    "library(mapview)\n",
    "library(ggplot2)\n",
    "library(ggspatial)\n",
    "\n",
    "# Read data\n",
    "parking<-read.csv(\"Parking.csv\")\n",
    "business<-read.csv(\"business.csv\")\n",
    "\n",
    "# Convert the business dataset to a spatial object\n",
    "business_sf <- st_as_sf(business, coords = c(\"long\", \"lat\"), crs = 4326)\n",
    "\n",
    "# Convert the parking dataset to a spatial object\n",
    "parking_sf <- st_as_sf(parking, coords = c(\"lng\", \"lat\"), crs = 4326)\n",
    "\n",
    "# Perform a spatial join to find the nearest business for each parking space\n",
    "nearest_business <- st_nearest_feature(parking_sf, business_sf)\n",
    "\n",
    "# Add the nearest business information to the parking dataset\n",
    "parking_data_with_nearest_business <- cbind(parking, nearest_business)\n",
    "\n",
    "#Display data on map\n",
    "map<-parking_data_with_nearest_business %>%\n",
    "  st_as_sf(\n",
    "    coords=c(\"lng\",\"lat\"),\n",
    "    crs=4326\n",
    "  )\n",
    "\n",
    "map %>% mapview()\n",
    "\n",
    "\n",
    "parking_data_with_nearest_business$traffic_volume = c(100, 150, 90, 50, 250, 75, 65, 40)\n",
    "\n",
    "\n",
    "# Define a scoring function\n",
    "calculate_score <- function(traffic_volume, lat, lng, capacity, nearest_businesses) {\n",
    "  # Define weights for each factor\n",
    "  weights <- c(0.3, 0.1, 0.1, 0.2, 0.3)\n",
    "  \n",
    "  # Normalize each factor\n",
    "  normalized_traffic <- (traffic_volume - min(traffic_volume)) / (max(traffic_volume) - min(traffic_volume))\n",
    "  normalized_capacity <- (capacity - min(capacity)) / (max(capacity) - min(capacity))\n",
    "  normalized_distance <- (nearest_businesses - min(nearest_businesses)) / (max(nearest_businesses) - min(nearest_businesses))\n",
    "  \n",
    "  # Calculate the score\n",
    "  score <- weights[1] * normalized_traffic +\n",
    "    weights[2] * normalized_capacity +\n",
    "    weights[3] * normalized_distance +\n",
    "    weights[4] * lat +\n",
    "    weights[5] * lng\n",
    "  \n",
    "  return(abs(score))\n",
    "}\n",
    "\n",
    "# Calculate score for each parking spot\n",
    "parking_data_with_nearest_business$score <- calculate_score(parking_data_with_nearest_business$traffic_volume, parking_data_with_nearest_business$lat, parking_data_with_nearest_business$lng, parking_data_with_nearest_business$capacity, parking_data_with_nearest_business$nearest_business)\n",
    "\n",
    "# Rank the parking spots based on the score\n",
    "ranked_data <- parking_data_with_nearest_business[order(-parking_data_with_nearest_business$score),]\n",
    "\n",
    "highest_score_index <- which.max(ranked_data$score)\n",
    "highest_score_parking_spot <- parking_data_with_nearest_business[highest_score_index, ]\n",
    "\n",
    "# Print the parking spot with the highest score\n",
    "print(highest_score_parking_spot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Result and Discussion\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
