{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading required package: opendatatoronto\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading required package: tidyjson\n",
      "\n",
      "\n",
      "Attaching package: 'tidyjson'\n",
      "\n",
      "\n",
      "The following object is masked from 'package:stats':\n",
      "\n",
      "    filter\n",
      "\n",
      "\n",
      "Loading required package: tidygeocoder\n",
      "\n",
      "Loading required package: sf\n",
      "\n",
      "Linking to GEOS 3.11.2, GDAL 3.8.2, PROJ 9.3.1; sf_use_s2() is TRUE\n",
      "\n",
      "Loading required package: ggspatial\n",
      "\n",
      "Loading required package: stringr\n",
      "\n",
      "Loading required package: dplyr\n",
      "\n",
      "\n",
      "Attaching package: 'dplyr'\n",
      "\n",
      "\n",
      "The following objects are masked from 'package:stats':\n",
      "\n",
      "    filter, lag\n",
      "\n",
      "\n",
      "The following objects are masked from 'package:base':\n",
      "\n",
      "    intersect, setdiff, setequal, union\n",
      "\n",
      "\n",
      "Loading required package: ggplot2\n",
      "\n",
      "Loading required package: tidyverse\n",
      "\n",
      "── \u001b[1mAttaching core tidyverse packages\u001b[22m ──────────────────────── tidyverse 2.0.0 ──\n",
      "\u001b[32m✔\u001b[39m \u001b[34mforcats  \u001b[39m 1.0.0     \u001b[32m✔\u001b[39m \u001b[34mreadr    \u001b[39m 2.1.4\n",
      "\u001b[32m✔\u001b[39m \u001b[34mlubridate\u001b[39m 1.9.2     \u001b[32m✔\u001b[39m \u001b[34mtibble   \u001b[39m 3.2.1\n",
      "\u001b[32m✔\u001b[39m \u001b[34mpurrr    \u001b[39m 1.0.2     \u001b[32m✔\u001b[39m \u001b[34mtidyr    \u001b[39m 1.3.0\n",
      "── \u001b[1mConflicts\u001b[22m ────────────────────────────────────────── tidyverse_conflicts() ──\n",
      "\u001b[31m✖\u001b[39m \u001b[34mdplyr\u001b[39m::\u001b[32mfilter()\u001b[39m masks \u001b[34mtidyjson\u001b[39m::filter(), \u001b[34mstats\u001b[39m::filter()\n",
      "\u001b[31m✖\u001b[39m \u001b[34mdplyr\u001b[39m::\u001b[32mlag()\u001b[39m    masks \u001b[34mstats\u001b[39m::lag()\n",
      "\u001b[36mℹ\u001b[39m Use the conflicted package (\u001b[3m\u001b[34m<http://conflicted.r-lib.org/>\u001b[39m\u001b[23m) to force all conflicts to become errors\n",
      "Loading required package: FNN\n",
      "\n",
      "Loading required package: mapview\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load all required libraries for the notebook, including data package\n",
    "if(!require(\"opendatatoronto\")) {\n",
    "    install.packages(\"opendatatoronto\")\n",
    "    library(opendatatoronto) # data source\n",
    "}\n",
    "if(!require(\"tidyjson\")) {\n",
    "    install.packages(\"tidyjson\")\n",
    "    library(tidyjson)\n",
    "}\n",
    "if(!require(\"tidygeocoder\")) {\n",
    "    install.packages(\"tidygeocoder\")\n",
    "    library(tidygeocoder)\n",
    "}\n",
    "if(!require(\"sf\")) {\n",
    "    install.packages(\"sf\")\n",
    "    library(sf)\n",
    "}\n",
    "\n",
    "if(!require(\"ggspatial\")) {\n",
    "    install.packages(\"ggspatial\")\n",
    "    library(ggspatial)\n",
    "}\n",
    "if(!require(\"stringr\")) {\n",
    "    install.packages(\"stringr\")\n",
    "    library(stringr)\n",
    "}\n",
    "if(!require(\"dplyr\")) {\n",
    "    install.packages(\"dplyr\")\n",
    "    library(dplyr)\n",
    "}\n",
    "if(!require(\"ggplot2\")) {\n",
    "    install.packages(\"ggplot2\")\n",
    "    library(ggplot2)\n",
    "}\n",
    "if(!require(\"tidyverse\")) {\n",
    "    install.packages(\"tidyverse\")\n",
    "    library(tidyverse)\n",
    "}\n",
    "if(!require(\"FNN\")) {\n",
    "    install.packages(\"FNN\")\n",
    "    library(FNN)\n",
    "}\n",
    "\n",
    "if(!require(\"mapview\")) {\n",
    "    install.packages(\"mapview\")\n",
    "    library(mapview)\n",
    "}\n",
    "\n",
    "if(!require(\"forecast\")) {\n",
    "    install.packages(\"forecast\")\n",
    "    library(forecast)\n",
    "}\n",
    "\n",
    "if(!require(\"purrr\")) {\n",
    "    install.packages(\"purrr\")\n",
    "    library(purrr)\n",
    "}\n",
    "\n",
    "if(!require(\"e1071\")) {\n",
    "    install.packages(\"e1071\")\n",
    "    library(e1071)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Get Data - Traffic\n",
    "# output Data Description:\n",
    "# Dataframe with all intersection and daily count ( peak 4 hours), including lng / lat\n",
    "\n",
    "# # ? and todo:\n",
    "# # is separate street name needed\n",
    "# # direction of the street to be determined. How?\n",
    "\n",
    "\n",
    "# # package_traffic <- show_package(\"traffic-volumes-at-intersections-for-all-modes\")\n",
    "\n",
    "# # get all resources for this package\n",
    "# resources <- list_package_resources(\"traffic-volumes-at-intersections-for-all-modes\")\n",
    "\n",
    "# # identify datastore resources; by default, Toronto Open Data sets datastore resource format to CSV for non-geospatial and GeoJSON for geospatial resources\n",
    "# datastore_resources <- filter(resources, tolower(format) %in% c(\"csv\", \"geojson\"))\n",
    "\n",
    "# # load data # The method of loading data directly using turns out to be insufficient as the get_resource() only returns first 32000 rows of record. \n",
    "# location <- filter(datastore_resources, row_number() == 1) %>% get_resource()\n",
    "# traffic1 <- filter(datastore_resources, row_number() == 3) %>% get_resource()\n",
    "# traffic2 <- filter(datastore_resources, row_number() == 4) %>% get_resource()\n",
    "# traffic3 <- filter(datastore_resources, row_number() == 5) %>% get_resource()\n",
    "# traffic4 <- filter(datastore_resources, row_number() == 6) %>% get_resource()\n",
    "# traffic5 <- filter(datastore_resources, row_number() == 7) %>% get_resource()\n",
    "\n",
    "# To Run the code download the raw files from \n",
    "# https://open.toronto.ca/dataset/traffic-volumes-at-intersections-for-all-modes/ \n",
    "# and save the files in .csv format to the path: ../Data/Toronto/Traffic, 7 Files below. \n",
    "# count_metadata.csv\n",
    "# locations.csv\n",
    "# raw-data-1980-1989.csv\n",
    "# raw-data-1990-1999.csv\n",
    "# raw-data-2000-2009.csv\n",
    "# raw-data-2010-2019.csv\n",
    "# raw-data-2020-2029.csv\n",
    "location <- read.csv(\"../Data/Toronto/Traffic/locations.csv\") # ensure these path are correct if you have to download the files manually. \n",
    "traffic1 <- read.csv(\"../Data/Toronto/Traffic/raw-data-1980-1989.csv\")\n",
    "traffic2 <- read.csv(\"../Data/Toronto/Traffic/raw-data-1990-1999.csv\")\n",
    "traffic3 <- read.csv(\"../Data/Toronto/Traffic/raw-data-2000-2009.csv\")\n",
    "traffic4 <- read.csv(\"../Data/Toronto/Traffic/raw-data-2010-2019.csv\")\n",
    "traffic5 <- read.csv(\"../Data/Toronto/Traffic/raw-data-2020-2029.csv\")\n",
    "all_traffic = bind_rows(traffic1,traffic2,traffic3,traffic4,traffic5) # combine all raw data into one data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# clean and transform load - Traffic Data\n",
    "# Output data for modelling CleanTraffic\n",
    "# define parameters for cleaning\n",
    "peakhours <- 4 # number of peak hours of data per day. value should be between 1 and 10\n",
    "\n",
    "# get full intersection volume for each intersection based on peak hours of each day. \n",
    "# get average vol per intersection per year. \n",
    "# get number of years list, sort from low to high\n",
    "\n",
    "# \n",
    "\n",
    "CleanTraffic <- all_traffic %>%\n",
    "  filter(centreline_type == 2) %>% # only need intersection data\n",
    "  mutate(counthour = str_extract(time_start, \"(?<=T)(\\\\d+)(?=\\\\:)\")) %>% # extract hour\n",
    "  mutate(total_int_traffic = sb_cars_r + sb_cars_t + sb_cars_l +\n",
    "    nb_cars_r + nb_cars_t + nb_cars_l + wb_cars_r + wb_cars_t +\n",
    "    wb_cars_l + eb_cars_r + eb_cars_t + eb_cars_l) %>% # get total sum\n",
    "  select(one_of(c(\n",
    "    \"count_date\", \"location_id\", \"location\", \"lng\", \"lat\", \"counthour\",\n",
    "    \"total_int_traffic\"\n",
    "  ))) %>% # remove raw attributes, retain aggregate only\n",
    "  group_by(across(all_of(c(\"count_date\", \"location_id\", \"location\", \"lng\", \"lat\", \"counthour\")))) %>%\n",
    "  summarise_at(\"total_int_traffic\",sum) %>% # agregate hourly volume\n",
    "  group_by(across(all_of(c(\"count_date\", \"location_id\", \"location\", \"lng\", \"lat\")))) %>%\n",
    "  slice_max(order_by = total_int_traffic, n = peakhours) %>% # filter top peak hour volume\n",
    "  group_by(across(all_of(c(\"count_date\", \"location_id\", \"location\", \"lng\", \"lat\")))) %>%\n",
    "  summarise_at(\"total_int_traffic\", sum)%>% # aggregate daily peak hour volume\n",
    "  mutate(count_date= as.Date(count_date, format(\"%Y-%m-%d\"))) %>% \n",
    "  mutate(year = as.numeric(format(count_date,'%Y'))) %>% #add year\n",
    "  group_by(across(all_of(c(\"year\", \"location_id\", \"location\", \"lat\",\"lng\")))) %>% # group by year to get the average per intersection per year\n",
    "  summarise(AvgTotal = mean(total_int_traffic), .groups = \"drop\")  # average traffic volume for that year and location\n",
    "traffic_years <- unique(CleanTraffic$year) # get number of years list, sort from low to high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".dl-inline {width: auto; margin:0; padding: 0}\n",
       ".dl-inline>dt, .dl-inline>dd {float: none; width: auto; display: inline-block}\n",
       ".dl-inline>dt::after {content: \":\\0020\"; padding-right: .5ex}\n",
       ".dl-inline>dt:not(:first-of-type) {padding-left: .5ex}\n",
       "</style><dl class=dl-inline><dt>address</dt><dd>'character'</dd><dt>lat</dt><dd>'character'</dd><dt>lng</dt><dd>'character'</dd><dt>carpark_type</dt><dd>'character'</dd><dt>rate_half_hr</dt><dd>'character'</dd><dt>capacity</dt><dd>'character'</dd><dt>rate</dt><dd>'character'</dd></dl>\n"
      ],
      "text/latex": [
       "\\begin{description*}\n",
       "\\item[address] 'character'\n",
       "\\item[lat] 'character'\n",
       "\\item[lng] 'character'\n",
       "\\item[carpark\\textbackslash{}\\_type] 'character'\n",
       "\\item[rate\\textbackslash{}\\_half\\textbackslash{}\\_hr] 'character'\n",
       "\\item[capacity] 'character'\n",
       "\\item[rate] 'character'\n",
       "\\end{description*}\n"
      ],
      "text/markdown": [
       "address\n",
       ":   'character'lat\n",
       ":   'character'lng\n",
       ":   'character'carpark_type\n",
       ":   'character'rate_half_hr\n",
       ":   'character'capacity\n",
       ":   'character'rate\n",
       ":   'character'\n",
       "\n"
      ],
      "text/plain": [
       "     address          lat          lng carpark_type rate_half_hr     capacity \n",
       " \"character\"  \"character\"  \"character\"  \"character\"  \"character\"  \"character\" \n",
       "        rate \n",
       " \"character\" "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".dl-inline {width: auto; margin:0; padding: 0}\n",
       ".dl-inline>dt, .dl-inline>dd {float: none; width: auto; display: inline-block}\n",
       ".dl-inline>dt::after {content: \":\\0020\"; padding-right: .5ex}\n",
       ".dl-inline>dt:not(:first-of-type) {padding-left: .5ex}\n",
       "</style><dl class=dl-inline><dt>address</dt><dd>'character'</dd><dt>lat</dt><dd>'numeric'</dd><dt>lng</dt><dd>'numeric'</dd><dt>carpark_type</dt><dd>'character'</dd><dt>rate_half_hr</dt><dd>'numeric'</dd><dt>capacity</dt><dd>'numeric'</dd><dt>rate</dt><dd>'character'</dd></dl>\n"
      ],
      "text/latex": [
       "\\begin{description*}\n",
       "\\item[address] 'character'\n",
       "\\item[lat] 'numeric'\n",
       "\\item[lng] 'numeric'\n",
       "\\item[carpark\\textbackslash{}\\_type] 'character'\n",
       "\\item[rate\\textbackslash{}\\_half\\textbackslash{}\\_hr] 'numeric'\n",
       "\\item[capacity] 'numeric'\n",
       "\\item[rate] 'character'\n",
       "\\end{description*}\n"
      ],
      "text/markdown": [
       "address\n",
       ":   'character'lat\n",
       ":   'numeric'lng\n",
       ":   'numeric'carpark_type\n",
       ":   'character'rate_half_hr\n",
       ":   'numeric'capacity\n",
       ":   'numeric'rate\n",
       ":   'character'\n",
       "\n"
      ],
      "text/plain": [
       "     address          lat          lng carpark_type rate_half_hr     capacity \n",
       " \"character\"    \"numeric\"    \"numeric\"  \"character\"    \"numeric\"    \"numeric\" \n",
       "        rate \n",
       " \"character\" "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "FALSE"
      ],
      "text/latex": [
       "FALSE"
      ],
      "text/markdown": [
       "FALSE"
      ],
      "text/plain": [
       "[1] FALSE"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get Green P Parking package from Open Data-Toronto\n",
    "package <- show_package(\"b66466c3-69c8-4825-9c8b-04b270069193\")\n",
    "\n",
    "data=as.data.frame('Green P Parking')  # read the dataset Green P Parking in the package \n",
    "data<-show_package(package)\n",
    "\n",
    "resources<-list_package_resources(package)\n",
    "\n",
    "# Identify resources\n",
    "data_resources <- filter(resources, tolower(format) %in% c(\"csv\", \"json\"))\n",
    "\n",
    "# Load Green P Parking 2019 data\n",
    "data <- filter(data_resources, row_number() == 1) %>% get_resource()\n",
    "\n",
    "# Extract required columns from main data\n",
    "data<- data.frame(address=data$carparks$address,\n",
    "                                     lat=data$carparks$lat,\n",
    "                                     lng=data$carparks$lng,\n",
    "                                      carpark_type=data$carparks$carpark_type_str,\n",
    "                                      rate_half_hr=data$carparks$rate_half_hour,\n",
    "                                      capacity=data$carparks$capacity,\n",
    "                                      rate=data$carparks$rate_half_hour\n",
    "                  )\n",
    "\n",
    "# Check class of each attribute\n",
    "sapply(data, class) \n",
    "\n",
    "# Convert char to numeric class\n",
    "data$lat<-as.numeric(data$lat)\n",
    "data$lng<-as.numeric(data$lng)\n",
    "data$rate_half_hr<-as.numeric(data$rate_half_hr)\n",
    "data$capacity<-as.numeric(data$capacity)\n",
    "\n",
    "sapply(data, class) \n",
    "\n",
    "# Check for missing values\n",
    "any(is.na(data))\n",
    "\n",
    "# Extract street name from address\n",
    "data <- data %>%\n",
    "  mutate(extracted_address = str_replace_all(data$address, \"\\\\(.*?\\\\)\",\"\"))\n",
    "\n",
    "data$extracted_address<-str_replace_all(data$extracted_address, \"-.*\", \"\")\n",
    "data$extracted_address<-str_replace_all(data$extracted_address, \",.*\", \"\")\n",
    "\n",
    "# Extract data with carpark_type as 'Surface'\n",
    "data <- data %>%\n",
    "  filter(carpark_type == \"Surface\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Extracting address, latitude, and longitude\n",
    "data <- data[, c(\"address\", \"lat\", \"lng\")]\n",
    "\n",
    "# Adding the new rows\n",
    "new_rows <- data.frame(\n",
    "  address = c(\n",
    "    \"365 Lippincott Street\",\n",
    "    \"35 Erindale Avenue\",\n",
    "    \"14 Arundel Avenue\",\n",
    "    \"265 Armadale Avenue\",\n",
    "    \"1612 Danforth Avenue\",\n",
    "    \"117 Hammersmith Avenue\",\n",
    "    \"166 Woodbine Ave\",\n",
    "    \"19 Spadina Road\",\n",
    "    \"2300 Lake shore Boulevard West\",\n",
    "    \"35 Erindale Avenue\",\n",
    "    \"265 Armadale Avenue\",\n",
    "    \"2300 Lake shore Boulevard West\"\n",
    "  ),\n",
    "  lat = c(\n",
    "    43.665054,\n",
    "    43.688543,\n",
    "    43.695882,\n",
    "    43.721568,\n",
    "    43.684903,\n",
    "    43.693819,\n",
    "    43.674124,\n",
    "    43.677754,\n",
    "    43.623545,\n",
    "    43.688543,\n",
    "    43.721568,\n",
    "    43.623545\n",
    "  ),\n",
    "  lng = c(\n",
    "    -79.409662,\n",
    "    -79.411305,\n",
    "    -79.42134,\n",
    "    -79.427191,\n",
    "    -79.325679,\n",
    "    -79.428246,\n",
    "    -79.319325,\n",
    "    -79.403948,\n",
    "    -79.479056,\n",
    "    -79.411305,\n",
    "    -79.427191,\n",
    "    -79.479056\n",
    "  )\n",
    ")\n",
    "\n",
    "# Adding the new rows to the extracted GreenPParking dataset\n",
    "data <- rbind(data, new_rows)\n",
    "\n",
    "# Creating the 'Convert' column\n",
    "data$Convert <- ifelse(data$address %in% new_rows$address, 1, 0)\n",
    "\n",
    "# Convert Lat/Lng to address\n",
    "geo_rev_data<-data %>%\n",
    "  tidygeocoder::reverse_geocode(\n",
    "    lat=lat,\n",
    "    long=lng,\n",
    "    method=\"osm\")\n",
    "\n",
    "\n",
    "\n",
    "#write.csv(data, \"GreenPParking.csv\",row.names = FALSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Combine Parking Data and Clean Traffic Data to prepare input data for Time Series Model to predict traffic / EV charging demand\n",
    "\n",
    "# loop through low to high year number\n",
    "# filter each year, to list of unique intersections with traffic volume\n",
    "# run knn algo against parking lot, find k nearst intersection\n",
    "# use knn nn.index to find the k intersection vol, get average, creating a vector\n",
    "# add vector to parking data as new column with year and volume\n",
    "# next loop for the next year\n",
    "\n",
    "#Output TS_Input\n",
    "\n",
    "K <- 5 # Parameter used in knn calculation. \n",
    "All_park <- data %>%\n",
    "    select(one_of(c(\"lat\",\"lng\"))) # location coordinates of all surface car parks. \n",
    "\n",
    "TS_Input <- data # TS_Input is the input data for the time series of traffic volume forecast. \n",
    "\n",
    "for (i in traffic_years) {\n",
    "    # print(paste(\"This is year\" ,i))\n",
    "    year_traffic<- filter(CleanTraffic, year == i) # get the traffic volume, intersection for each year.\n",
    "    All_int <-select(year_traffic,one_of(c(\"lat\",\"lng\"))) # extract just the coordinates for K nearest neighter calculation\n",
    "    knn_dist <- get.knnx(All_int, All_park, k=K, algorithm=\"kd_tree\") # get the K nearest intersection to the parking lot and their index list\n",
    "    \n",
    "    get.mean <- function(x) { # custom function defined to calculate the average of the K nearest intersection volume\n",
    "    mean(slice(year_traffic, c(x))$AvgTotal)\n",
    "    }\n",
    "\n",
    "    TS_Input[paste0(\"NearbyTraffic_\",i)]  <- apply(knn_dist$nn.index,1,get.mean) # calculate the avaerage per row of indices\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Model 1 - Time Series Forecast\n",
    "\n",
    "# forecast traffic for 2024 given the TS_Input\n",
    "# apply % of EV vehicle multiplier for 2022 and predicted year 2024 to get amount of estimated EV traffic.\n",
    "# data frame that has the original location, EV traffic for 2022, and predicted EV traffic for 2024\n",
    "\n",
    "\n",
    "library(forecast)\n",
    "library(purrr)\n",
    "\n",
    "# Time series model and Forecast for 2024\n",
    "arima_models <- apply(TS_Input[, -c(1,2,3,4,5,6,7,8)], 1, auto.arima)\n",
    "model_forecasts_2024 <- lapply(arima_models, function(x) forecast(x, h = 1))\n",
    "\n",
    "TS_Output_2024 <- map_dbl(model_forecasts_2024, \"mean\")\n",
    "TS_Output_2024 <- round(TS_Output_2024)\n",
    "\n",
    "\n",
    "# Format output for 2024\n",
    "# Data frame that contains the original location, lat, and lng columns,\n",
    "# % of EV traffic for 2022 and % of EV traffic for predicted 2024\n",
    "\n",
    "head(TS_Output_2024)\n",
    "\n",
    "# % of EV vehicle multiplier for 2022\n",
    "mulitplier_2022 = .03\n",
    "\n",
    "# % of EV Vehicle multiplier for predicted year \n",
    "multiplier_predicted = .0533\n",
    "\n",
    "TS_Output_df = data.frame(TS_Input$address,TS_Input$lat,TS_Input$lng, TS_Input$NearbyTraffic_2022,TS_Output_2024)\n",
    "#head(TS_Output_df)\n",
    "\n",
    "# Apply % of EV Vehicle multiplier to 2022 and predicted year 2024, present a new columns to Df\n",
    "traffic_2022 <- data.frame(TS_Output_df$TS_Input.NearbyTraffic_2022)\n",
    "traffic_2024 <-data.frame(TS_Output_df$TS_Output_2024)\n",
    "\n",
    "TS_Output_df$EV_nearbyTraffic_2022 <- apply(traffic_2022, 1, function(x) round(x * mulitplier_2022))\n",
    "TS_Output_df$EV_nearbyTraffic_2024 <- apply(traffic_2024, 1, function(x) round(x * multiplier_predicted))\n",
    "\n",
    "\n",
    "# Final Output\n",
    "head(TS_Output_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Model 2 - SVM\n",
    "\n",
    "# Pre-processing\n",
    "\n",
    "# Read data\n",
    "parking_data <- data\n",
    "business_data <- read.csv(\"business.csv\")\n",
    "\n",
    "K <- 8  # Number of nearest businesses to consider\n",
    "\n",
    "for (i in 1:nrow(parking_data)) {\n",
    "    parking_spot <- parking_data[i, c(\"lat\", \"lng\")]  # Coordinates of the parking spot\n",
    "    nearest_biz_indices <- get.knnx(business_data[, c(\"lat\", \"long\")], parking_spot, k = K, algorithm = \"kd_tree\")$nn.index\n",
    "    nearest_biz_distances <- get.knnx(business_data[, c(\"lat\", \"long\")], parking_spot, k = K, algorithm = \"kd_tree\")$nn.dist\n",
    "    \n",
    "    # Calculate the number of nearest businesses and their distances\n",
    "    num_nearest_biz <- length(nearest_biz_indices)\n",
    "    avg_distance <- mean(nearest_biz_distances)\n",
    "    \n",
    "    nearest_biz_customers <- sapply(nearest_biz_indices, function(idx) {\n",
    "  mean(business_data[idx, \"qCustomer\"])\n",
    "})\n",
    "    nearest_biz_time_spent <- sapply(nearest_biz_indices, function(idx) {\n",
    "  mean(business_data[idx, \"tCustomer\"])\n",
    "})\n",
    "\n",
    "    overall_mean_customers <- mean(nearest_biz_customers)\n",
    "    overall_mean_time_spent <- mean(nearest_biz_time_spent)\n",
    "\n",
    "\n",
    "    # Add the results to the parking_data dataframe\n",
    "    parking_data[i, paste0(\"nearest_biz_count_\", K)] <- num_nearest_biz\n",
    "    parking_data[i, paste0(\"avg_distance_to_biz_\", K)] <- avg_distance\n",
    "    parking_data[i, paste0(\"avg_customers_nearby_\", K)] <- mean(overall_mean_customers)\n",
    "    parking_data[i, paste0(\"avg_time_spent_nearby_\", K)] <- mean(overall_mean_time_spent)\n",
    "}\n",
    "\n",
    "# Add EV Traffic data\n",
    "parking_data$traffic_volume=TS_Output_df$EV_nearbyTraffic_2022\t\n",
    "\n",
    "parking_data <- parking_data %>%\n",
    "  group_by(address) %>%\n",
    "  summarise (distance = mean(avg_distance_to_biz_8), n_business = mean(nearest_biz_count_8),n_customers = mean(avg_customers_nearby_8), time_spent= mean(avg_time_spent_nearby_8), traffic_volume = traffic_volume, convert= mean(Convert)) \n",
    "\n",
    "# Normalize predictors\n",
    "parking_data$distance <- scale(parking_data$distance)\n",
    "parking_data$traffic_volume <- scale(parking_data$traffic_volume)\n",
    "parking_data$time_spent<- scale(parking_data$time_spent)\n",
    "parking_data$n_customers<- scale(parking_data$n_customers)\n",
    "\n",
    "set.seed(123)  # for reproducibility\n",
    "\n",
    "train_indices <- sample(1:nrow(filtered_result_df3), 0.7 * nrow(parking_data))  # 70% for training\n",
    "train_data <- parking_data[train_indices, ]\n",
    "test_data <- parking_data[-train_indices, ]\n",
    "\n",
    "# Create a formula including predictor columns\n",
    "formula <- as.formula(\"convert ~ distance + traffic_volume \")\n",
    "\n",
    "# Create the SVM model with the formula\n",
    "svm_model <- svm(formula, data = train_data, kernel = \"linear\")\n",
    "\n",
    "# Get the coefficients (weights) of the model\n",
    "weights <- coef(svm_model)\n",
    "\n",
    "# Coefficients from the SVM model\n",
    "coefficients <- coef(svm_model)[-1]  # Exclude the intercept\n",
    "\n",
    "# Select the columns for which to calculate the score\n",
    "cols <- c(\"distance\", \"traffic_volume\")\n",
    "\n",
    "# Calculate the score for each row\n",
    "parking_data$score <- rowSums(parking_data[cols] * coefficients)\n",
    "\n",
    "\n",
    "# Sort the rows in descending order according to the score\n",
    "sorted_result <- parking_data %>%\n",
    "  arrange(desc(score))\n",
    "\n",
    "# View the sorted result\n",
    "head(sorted_result,12)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Get Data - Intersection\n",
    "# output Data Description:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Get Data - Business\n",
    "# output Data Description: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Define Region of Interest - Boundary\n",
    "## coordinates manually looked up from location dataset\n",
    "# 1406\t5370\tDUPONT ST AT OSSINGTON AVE (PX 842)\t-79.429019\t43.670031996501194\n",
    "# 251\t4180\tDUPONT ST AT SPADINA RD (PX 840)\t-79.407122\t43.67485699954096\n",
    "# 1885\t5864\tCOLLEGE ST AT OSSINGTON AVE (PX 829)\t-79.422705\t43.65439999619167\n",
    "# 241\t4170\tCOLLEGE ST AT SPADINA AVE (PX 279)\t-79.400048\t43.65794800150128\n",
    "\n",
    "# Input\n",
    "# Output\n",
    "\n",
    "boundary <- location %>%\n",
    "  select(location_id, location, lng, lat) %>%\n",
    "  filter(location_id %in% list(5370, 4180, 5864, 4170)) # boundary intersection ID\n",
    "\n",
    "lng_min <- min(boundary$lng) # west most value since it's negative\n",
    "lng_max <- max(boundary$lng) # east most value\n",
    "lat_min <- min(boundary$lat) # south most value\n",
    "lat_max <- max(boundary$lat) # north most value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Result and Discussion\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
