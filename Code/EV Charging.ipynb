{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all required libraries for the notebook, including data package\n",
    "if (!require(\"opendatatoronto\")) {\n",
    "    install.packages(\"opendatatoronto\")\n",
    "    library(opendatatoronto)\n",
    "}\n",
    "library(dplyr)\n",
    "library(stringr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Data - Traffice\n",
    "# output Data Description:\n",
    "# Dataframe with all intersection and daily count ( peak 4 hours), including lng / lat\n",
    "\n",
    "# ? and todo:\n",
    "# is separate street name needed\n",
    "# direction of the street to be determined. How?\n",
    "\n",
    "\n",
    "# package_traffic <- show_package(\"traffic-volumes-at-intersections-for-all-modes\")\n",
    "\n",
    "# get all resources for this package\n",
    "resources <- list_package_resources(\"traffic-volumes-at-intersections-for-all-modes\")\n",
    "\n",
    "# identify datastore resources; by default, Toronto Open Data sets datastore resource format to CSV for non-geospatial and GeoJSON for geospatial resources\n",
    "datastore_resources <- filter(resources, tolower(format) %in% c(\"csv\", \"geojson\"))\n",
    "\n",
    "# load data\n",
    "location <- filter(datastore_resources, row_number() == 1) %>% get_resource()\n",
    "traffic1 <- filter(datastore_resources, row_number() == 3) %>% get_resource()\n",
    "traffic2 <- filter(datastore_resources, row_number() == 4) %>% get_resource()\n",
    "traffic3 <- filter(datastore_resources, row_number() == 5) %>% get_resource()\n",
    "traffic4 <- filter(datastore_resources, row_number() == 6) %>% get_resource()\n",
    "traffic5 <- filter(datastore_resources, row_number() == 7) %>% get_resource()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean and transform load - Traffic Data\n",
    "# Output data for modelling CleanTraffic\n",
    "# define parameters for cleaning\n",
    "peakhours <- 4 # number of peak hours of data per day. value should be between 1 and 10\n",
    "\n",
    "\n",
    "clean_T1 <- traffic1 %>%\n",
    "  select(one_of(c(\n",
    "    \"count_date\", \"location_id\", \"location\", \"lng\", \"lat\", \"centreline_type\",\n",
    "    \"time_start\", \"sb_cars_r\", \"sb_cars_t\", \"sb_cars_l\",\n",
    "    \"nb_cars_r\", \"nb_cars_t\", \"nb_cars_l\", \"wb_cars_r\", \"wb_cars_t\", \"wb_cars_l\",\n",
    "    \"eb_cars_r\", \"eb_cars_t\", \"eb_cars_l\"\n",
    "  ))) %>% # select needed attributes\n",
    "  filter(centreline_type == 2) %>% # only need intersection data\n",
    "  mutate(counthour = str_extract(time_start, \"(?<=T)(\\\\d+)(?=\\\\:)\")) %>% # extract hour\n",
    "  mutate(total_int_traffic = sb_cars_r + sb_cars_t + sb_cars_l +\n",
    "    nb_cars_r + nb_cars_t + nb_cars_l + wb_cars_r + wb_cars_t +\n",
    "    wb_cars_l + eb_cars_r + eb_cars_t + eb_cars_l) %>% # get total sum\n",
    "  mutate(nb_exit_traffic = nb_cars_t + eb_cars_l + wb_cars_r) %>% # get north bound exit volume\n",
    "  mutate(sb_exit_traffic = sb_cars_t + eb_cars_r + wb_cars_l) %>% # get south bound exit volume\n",
    "  mutate(wb_exit_traffic = wb_cars_t + nb_cars_l + sb_cars_r) %>% # get west bound exit volume\n",
    "  mutate(eb_exit_traffic = eb_cars_t + nb_cars_r + sb_cars_l) %>% # get east bound exit volume\n",
    "  select(one_of(c(\n",
    "    \"count_date\", \"location_id\", \"location\", \"lng\", \"lat\", \"counthour\",\n",
    "    \"total_int_traffic\", \"nb_exit_traffic\", \"sb_exit_traffic\", \"wb_exit_traffic\",\n",
    "    \"eb_exit_traffic\"\n",
    "  ))) %>% # remove raw attributes, retain aggregate only\n",
    "  group_by(across(all_of(c(\"count_date\", \"location_id\", \"location\", \"lng\", \"lat\", \"counthour\")))) %>%\n",
    "  summarise(across(any_of(c(\n",
    "    \"total_int_traffic\", \"nb_exit_traffic\", \"sb_exit_traffic\", \"wb_exit_traffic\",\n",
    "    \"eb_exit_traffic\"\n",
    "  )), sum)) %>% # agregate hourly volume\n",
    "  group_by(across(all_of(c(\"count_date\", \"location_id\", \"location\", \"lng\", \"lat\")))) %>%\n",
    "  slice_max(order_by = total_int_traffic, n = peakhours) %>% # filter top peak hour volume\n",
    "  group_by(across(all_of(c(\"count_date\", \"location_id\", \"location\", \"lng\", \"lat\")))) %>%\n",
    "  summarise(across(any_of(c(\n",
    "    \"total_int_traffic\", \"nb_exit_traffic\", \"sb_exit_traffic\", \"wb_exit_traffic\",\n",
    "    \"eb_exit_traffic\"\n",
    "  )), sum)) # aggregate daily peak hour volume\n",
    "clean_T2 <- traffic2 %>%\n",
    "  select(one_of(c(\n",
    "    \"count_date\", \"location_id\", \"location\", \"lng\", \"lat\", \"centreline_type\",\n",
    "    \"time_start\", \"sb_cars_r\", \"sb_cars_t\", \"sb_cars_l\",\n",
    "    \"nb_cars_r\", \"nb_cars_t\", \"nb_cars_l\", \"wb_cars_r\", \"wb_cars_t\", \"wb_cars_l\",\n",
    "    \"eb_cars_r\", \"eb_cars_t\", \"eb_cars_l\"\n",
    "  ))) %>% # select needed attributes\n",
    "  filter(centreline_type == 2) %>% # only need intersection data\n",
    "  mutate(counthour = str_extract(time_start, \"(?<=T)(\\\\d+)(?=\\\\:)\")) %>% # extract hour\n",
    "  mutate(total_int_traffic = sb_cars_r + sb_cars_t + sb_cars_l +\n",
    "    nb_cars_r + nb_cars_t + nb_cars_l + wb_cars_r + wb_cars_t +\n",
    "    wb_cars_l + eb_cars_r + eb_cars_t + eb_cars_l) %>% # get total sum\n",
    "  mutate(nb_exit_traffic = nb_cars_t + eb_cars_l + wb_cars_r) %>% # get north bound exit volume\n",
    "  mutate(sb_exit_traffic = sb_cars_t + eb_cars_r + wb_cars_l) %>% # get south bound exit volume\n",
    "  mutate(wb_exit_traffic = wb_cars_t + nb_cars_l + sb_cars_r) %>% # get west bound exit volume\n",
    "  mutate(eb_exit_traffic = eb_cars_t + nb_cars_r + sb_cars_l) %>% # get east bound exit volume\n",
    "  select(one_of(c(\n",
    "    \"count_date\", \"location_id\", \"location\", \"lng\", \"lat\", \"counthour\",\n",
    "    \"total_int_traffic\", \"nb_exit_traffic\", \"sb_exit_traffic\", \"wb_exit_traffic\",\n",
    "    \"eb_exit_traffic\"\n",
    "  ))) %>% # remove raw attributes, retain aggsregate only\n",
    "  group_by(across(all_of(c(\"count_date\", \"location_id\", \"location\", \"lng\", \"lat\", \"counthour\")))) %>%\n",
    "  summarise(across(any_of(c(\n",
    "    \"total_int_traffic\", \"nb_exit_traffic\", \"sb_exit_traffic\", \"wb_exit_traffic\",\n",
    "    \"eb_exit_traffic\"\n",
    "  )), sum)) %>% # agregate hourly volume\n",
    "  group_by(across(all_of(c(\"count_date\", \"location_id\", \"location\", \"lng\", \"lat\")))) %>%\n",
    "  slice_max(order_by = total_int_traffic, n = peakhours) %>% # filter top peak hour volume\n",
    "  group_by(across(all_of(c(\"count_date\", \"location_id\", \"location\", \"lng\", \"lat\")))) %>%\n",
    "  summarise(across(any_of(c(\n",
    "    \"total_int_traffic\", \"nb_exit_traffic\", \"sb_exit_traffic\", \"wb_exit_traffic\",\n",
    "    \"eb_exit_traffic\"\n",
    "  )), sum)) # aggregate daily peak hour volume\n",
    "\n",
    "clean_T3 <- traffic3 %>%\n",
    "  select(one_of(c(\n",
    "    \"count_date\", \"location_id\", \"location\", \"lng\", \"lat\", \"centreline_type\",\n",
    "    \"time_start\", \"sb_cars_r\", \"sb_cars_t\", \"sb_cars_l\",\n",
    "    \"nb_cars_r\", \"nb_cars_t\", \"nb_cars_l\", \"wb_cars_r\", \"wb_cars_t\", \"wb_cars_l\",\n",
    "    \"eb_cars_r\", \"eb_cars_t\", \"eb_cars_l\"\n",
    "  ))) %>% # select needed attributes\n",
    "  filter(centreline_type == 2) %>% # only need intersection data\n",
    "  mutate(counthour = str_extract(time_start, \"(?<=T)(\\\\d+)(?=\\\\:)\")) %>% # extract hour\n",
    "  mutate(total_int_traffic = sb_cars_r + sb_cars_t + sb_cars_l +\n",
    "    nb_cars_r + nb_cars_t + nb_cars_l + wb_cars_r + wb_cars_t +\n",
    "    wb_cars_l + eb_cars_r + eb_cars_t + eb_cars_l) %>% # get total sum\n",
    "  mutate(nb_exit_traffic = nb_cars_t + eb_cars_l + wb_cars_r) %>% # get north bound exit volume\n",
    "  mutate(sb_exit_traffic = sb_cars_t + eb_cars_r + wb_cars_l) %>% # get south bound exit volume\n",
    "  mutate(wb_exit_traffic = wb_cars_t + nb_cars_l + sb_cars_r) %>% # get west bound exit volume\n",
    "  mutate(eb_exit_traffic = eb_cars_t + nb_cars_r + sb_cars_l) %>% # get east bound exit volume\n",
    "  select(one_of(c(\n",
    "    \"count_date\", \"location_id\", \"location\", \"lng\", \"lat\", \"counthour\",\n",
    "    \"total_int_traffic\", \"nb_exit_traffic\", \"sb_exit_traffic\", \"wb_exit_traffic\",\n",
    "    \"eb_exit_traffic\"\n",
    "  ))) %>% # remove raw attributes, retain aggregate only\n",
    "  group_by(across(all_of(c(\"count_date\", \"location_id\", \"location\", \"lng\", \"lat\", \"counthour\")))) %>%\n",
    "  summarise(across(any_of(c(\n",
    "    \"total_int_traffic\", \"nb_exit_traffic\", \"sb_exit_traffic\", \"wb_exit_traffic\",\n",
    "    \"eb_exit_traffic\"\n",
    "  )), sum)) %>% # agregate hourly volume\n",
    "  group_by(across(all_of(c(\"count_date\", \"location_id\", \"location\", \"lng\", \"lat\")))) %>%\n",
    "  slice_max(order_by = total_int_traffic, n = peakhours) %>% # filter top peak hour volume\n",
    "  group_by(across(all_of(c(\"count_date\", \"location_id\", \"location\", \"lng\", \"lat\")))) %>%\n",
    "  summarise(across(any_of(c(\n",
    "    \"total_int_traffic\", \"nb_exit_traffic\", \"sb_exit_traffic\", \"wb_exit_traffic\",\n",
    "    \"eb_exit_traffic\"\n",
    "  )), sum)) # aggregate daily peak hour volume\n",
    "\n",
    "clean_T4 <- traffic4 %>%\n",
    "  select(one_of(c(\n",
    "    \"count_date\", \"location_id\", \"location\", \"lng\", \"lat\", \"centreline_type\",\n",
    "    \"time_start\", \"sb_cars_r\", \"sb_cars_t\", \"sb_cars_l\",\n",
    "    \"nb_cars_r\", \"nb_cars_t\", \"nb_cars_l\", \"wb_cars_r\", \"wb_cars_t\", \"wb_cars_l\",\n",
    "    \"eb_cars_r\", \"eb_cars_t\", \"eb_cars_l\"\n",
    "  ))) %>% # select needed attributes\n",
    "  filter(centreline_type == 2) %>% # only need intersection data\n",
    "  mutate(counthour = str_extract(time_start, \"(?<=T)(\\\\d+)(?=\\\\:)\")) %>% # extract hour\n",
    "  mutate(total_int_traffic = sb_cars_r + sb_cars_t + sb_cars_l +\n",
    "    nb_cars_r + nb_cars_t + nb_cars_l + wb_cars_r + wb_cars_t +\n",
    "    wb_cars_l + eb_cars_r + eb_cars_t + eb_cars_l) %>% # get total sum\n",
    "  mutate(nb_exit_traffic = nb_cars_t + eb_cars_l + wb_cars_r) %>% # get north bound exit volume\n",
    "  mutate(sb_exit_traffic = sb_cars_t + eb_cars_r + wb_cars_l) %>% # get south bound exit volume\n",
    "  mutate(wb_exit_traffic = wb_cars_t + nb_cars_l + sb_cars_r) %>% # get west bound exit volume\n",
    "  mutate(eb_exit_traffic = eb_cars_t + nb_cars_r + sb_cars_l) %>% # get east bound exit volume\n",
    "  select(one_of(c(\n",
    "    \"count_date\", \"location_id\", \"location\", \"lng\", \"lat\", \"counthour\",\n",
    "    \"total_int_traffic\", \"nb_exit_traffic\", \"sb_exit_traffic\", \"wb_exit_traffic\",\n",
    "    \"eb_exit_traffic\"\n",
    "  ))) %>% # remove raw attributes, retain aggregate only\n",
    "  group_by(across(all_of(c(\"count_date\", \"location_id\", \"location\", \"lng\", \"lat\", \"counthour\")))) %>%\n",
    "  summarise(across(any_of(c(\n",
    "    \"total_int_traffic\", \"nb_exit_traffic\", \"sb_exit_traffic\", \"wb_exit_traffic\",\n",
    "    \"eb_exit_traffic\"\n",
    "  )), sum)) %>% # agregate hourly volume\n",
    "  group_by(across(all_of(c(\"count_date\", \"location_id\", \"location\", \"lng\", \"lat\")))) %>%\n",
    "  slice_max(order_by = total_int_traffic, n = peakhours) %>% # filter top peak hour volume\n",
    "  group_by(across(all_of(c(\"count_date\", \"location_id\", \"location\", \"lng\", \"lat\")))) %>%\n",
    "  summarise(across(any_of(c(\n",
    "    \"total_int_traffic\", \"nb_exit_traffic\", \"sb_exit_traffic\", \"wb_exit_traffic\",\n",
    "    \"eb_exit_traffic\"\n",
    "  )), sum)) # aggregate daily peak hour volume\n",
    "\n",
    "clean_T5 <- traffic5 %>%\n",
    "  select(one_of(c(\n",
    "    \"count_date\", \"location_id\", \"location\", \"lng\", \"lat\", \"centreline_type\",\n",
    "    \"time_start\", \"sb_cars_r\", \"sb_cars_t\", \"sb_cars_l\",\n",
    "    \"nb_cars_r\", \"nb_cars_t\", \"nb_cars_l\", \"wb_cars_r\", \"wb_cars_t\", \"wb_cars_l\",\n",
    "    \"eb_cars_r\", \"eb_cars_t\", \"eb_cars_l\"\n",
    "  ))) %>% # select needed attributes\n",
    "  filter(centreline_type == 2) %>% # only need intersection data\n",
    "  mutate(counthour = str_extract(time_start, \"(?<=T)(\\\\d+)(?=\\\\:)\")) %>% # extract hour\n",
    "  mutate(total_int_traffic = sb_cars_r + sb_cars_t + sb_cars_l +\n",
    "    nb_cars_r + nb_cars_t + nb_cars_l + wb_cars_r + wb_cars_t +\n",
    "    wb_cars_l + eb_cars_r + eb_cars_t + eb_cars_l) %>% # get total sum\n",
    "  mutate(nb_exit_traffic = nb_cars_t + eb_cars_l + wb_cars_r) %>% # get north bound exit volume\n",
    "  mutate(sb_exit_traffic = sb_cars_t + eb_cars_r + wb_cars_l) %>% # get south bound exit volume\n",
    "  mutate(wb_exit_traffic = wb_cars_t + nb_cars_l + sb_cars_r) %>% # get west bound exit volume\n",
    "  mutate(eb_exit_traffic = eb_cars_t + nb_cars_r + sb_cars_l) %>% # get east bound exit volume\n",
    "  select(one_of(c(\n",
    "    \"count_date\", \"location_id\", \"location\", \"lng\", \"lat\", \"counthour\",\n",
    "    \"total_int_traffic\", \"nb_exit_traffic\", \"sb_exit_traffic\", \"wb_exit_traffic\",\n",
    "    \"eb_exit_traffic\"\n",
    "  ))) %>% # remove raw attributes, retain aggregate only\n",
    "  group_by(across(all_of(c(\"count_date\", \"location_id\", \"location\", \"lng\", \"lat\", \"counthour\")))) %>%\n",
    "  summarise(across(any_of(c(\n",
    "    \"total_int_traffic\", \"nb_exit_traffic\", \"sb_exit_traffic\", \"wb_exit_traffic\",\n",
    "    \"eb_exit_traffic\"\n",
    "  )), sum)) %>% # agregate hourly volume\n",
    "  group_by(across(all_of(c(\"count_date\", \"location_id\", \"location\", \"lng\", \"lat\")))) %>%\n",
    "  slice_max(order_by = total_int_traffic, n = peakhours) %>% # filter top peak hour volume\n",
    "  group_by(across(all_of(c(\"count_date\", \"location_id\", \"location\", \"lng\", \"lat\")))) %>%\n",
    "  summarise(across(any_of(c(\n",
    "    \"total_int_traffic\", \"nb_exit_traffic\", \"sb_exit_traffic\", \"wb_exit_traffic\",\n",
    "    \"eb_exit_traffic\"\n",
    "  )), sum)) # aggregate daily peak hour volume\n",
    "\n",
    "\n",
    "CleanTraffic <- bind_rows(clean_T1, clean_T2, clean_T3, clean_T4, clean_T5)\n",
    "head(CleanTraffic)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Data - Green P Parking\n",
    "# output Data Description:\n",
    "\n",
    "\n",
    "install.packages(\"tidyjson\")\n",
    "install.packages(\"tidygeocoder\")\n",
    "install.packages(\"sf\")\n",
    "install.packages(\"mapview\")\n",
    "\n",
    "library(tidyjson)\n",
    "library(dplyr)\n",
    "library(tidyverse)\n",
    "library(tidygeocoder)\n",
    "library(sf)\n",
    "library(mapview)\n",
    "library(stringr)\n",
    "\n",
    "\n",
    "# Read the JSON data\n",
    "json_data <- jsonlite::fromJSON(\"green-p-parking-2019.json\")\n",
    "\n",
    "# Convert the R object to a data frame\n",
    "df <- as.data.frame(json_data)\n",
    "\n",
    "# View the data frame\n",
    "head(df)\n",
    "\n",
    "\n",
    "# Extract required columns from main data\n",
    "data <- data.frame(\n",
    "  address = df$carparks.address,\n",
    "  lat = df$carparks.lat,\n",
    "  lng = df$carparks.lng,\n",
    "  carpark_type = df$carparks.carpark_type_str,\n",
    "  rate_half_hr = df$carparks.rate_half_hour,\n",
    "  capacity = df$carparks.capacity\n",
    ")\n",
    "\n",
    "# Check class of each attribute\n",
    "sapply(data, class)\n",
    "\n",
    "# Convert char to numeric class\n",
    "data$lat <- as.numeric(data$lat)\n",
    "data$lng <- as.numeric(data$lng)\n",
    "data$rate_half_hr <- as.numeric(data$rate_half_hr)\n",
    "data$capacity <- as.numeric(data$capacity)\n",
    "\n",
    "\n",
    "sapply(data, class)\n",
    "\n",
    "# Check for missing values\n",
    "any(is.na(data))\n",
    "\n",
    "\n",
    "# Extract street name from address\n",
    "data <- data %>%\n",
    "  mutate(extracted_address = str_replace_all(data$address, \"\\\\(.*?\\\\)\", \"\"))\n",
    "\n",
    "data$extracted_address <- str_replace_all (data$extracted_address, \"-.*\", \"\")\n",
    "data$extracted_address <- str_replace_all (data$extracted_address, \",.*\", \"\")\n",
    "\n",
    "# Extract data with carpark_type as 'Surface'\n",
    "data <- data %>%\n",
    "  filter(carpark_type == \"Surface\")\n",
    "\n",
    "\n",
    "# Convert Lat/Lng to address\n",
    "geo_rev_data <- data %>%\n",
    "  tidygeocoder::reverse_geocode(\n",
    "    lat = lat,\n",
    "    long = lng,\n",
    "    method = \"osm\"\n",
    "  )\n",
    "\n",
    "# Plot map\n",
    "map <- data %>%\n",
    "  st_as_sf(\n",
    "    coords = c(\"lng\", \"lat\"),\n",
    "    crs = 4326\n",
    "  )\n",
    "\n",
    "map %>% mapview()\n",
    "\n",
    "# Filter data to extract only parking spots with M5C pin code\n",
    "filtered_df <- geo_rev_data %>%\n",
    "  filter(grepl(\"M6G|M6H|M6J|M5T|M5S|M5R|M4V\", address...8))\n",
    "\n",
    "# Print the filtered data frame\n",
    "print(filtered_df)\n",
    "\n",
    "# Convert Lat/Lng to address and plot map\n",
    "filtered_df_map <- filtered_df %>%\n",
    "  st_as_sf(\n",
    "    coords = c(\"lng\", \"lat\"),\n",
    "    crs = 4326\n",
    "  )\n",
    "\n",
    "filtered_df_map %>% mapview()\n",
    "\n",
    "# Extract street names\n",
    "\n",
    "filtered_df_map$streets <- gsub(\"\\\\d+\", \"\", filtered_df_map$extracted_address) # Remove numbers # nolint # nolint\n",
    "filtered_df_map$streets <- gsub(\"\\\\.$\", \"\", filtered_df_map$streets) # Remove trailing periods # nolint\n",
    "\n",
    "filtered_df_map$lat <- filtered_df$lat\n",
    "\n",
    "filtered_df_map$lng <- filtered_df$lng\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'library' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mlibrary\u001b[49m(sf)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Define the polygon coordinates\u001b[39;00m\n\u001b[0;32m      4\u001b[0m polygon_coords \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m-\u001b[39m matrix(c(\n\u001b[0;32m      5\u001b[0m   \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m79.4289964889767\u001b[39m, \u001b[38;5;241m43.6700360241176\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m79.4226792245877\u001b[39m, \u001b[38;5;241m43.6543887000655\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m79.4000484228339\u001b[39m, \u001b[38;5;241m43.657948514946\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m79.4070875693025\u001b[39m, \u001b[38;5;241m43.6748646586934\u001b[39m,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m79.4289964889767\u001b[39m, \u001b[38;5;241m43.6700360241176\u001b[39m  \u001b[38;5;66;03m# Repeat first point to close the polygon exactly\u001b[39;00m\n\u001b[0;32m      6\u001b[0m ), ncol \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m, byrow \u001b[38;5;241m=\u001b[39m TRUE)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'library' is not defined"
     ]
    }
   ],
   "source": [
    "library(sf)\n",
    "\n",
    "# Define the polygon coordinates\n",
    "polygon_coords <- matrix(c(\n",
    "  -79.4289964889767, 43.6700360241176, -79.4226792245877, 43.6543887000655, -79.4000484228339, 43.657948514946, -79.4070875693025, 43.6748646586934,-79.4289964889767, 43.6700360241176  # Repeat first point to close the polygon exactly\n",
    "), ncol = 2, byrow = TRUE)\n",
    "\n",
    "# Create a polygon geometry\n",
    "polygon <- st_polygon(list(polygon_coords))\n",
    "polygon <- st_sfc(polygon, crs = 4326)\n",
    "\n",
    "# Convert data to sf object\n",
    "data_sf <- st_as_sf(filtered_df_map, coords = c(\"lng\", \"lat\"), crs = 4326)\n",
    "\n",
    "# Filter rows that intersect with the polygon\n",
    "data_within_polygon <- data_sf[st_intersects(data_sf, polygon), ]\n",
    "\n",
    "# Print the filtered data\n",
    "print(data_within_polygon)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Data - Intersection\n",
    "# output Data Description:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Data - Business\n",
    "# output Data Description: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Region of Interest - Boundary\n",
    "## coordinates manually looked up from location dataset\n",
    "# 1406\t5370\tDUPONT ST AT OSSINGTON AVE (PX 842)\t-79.429019\t43.670031996501194\n",
    "# 251\t4180\tDUPONT ST AT SPADINA RD (PX 840)\t-79.407122\t43.67485699954096\n",
    "# 1885\t5864\tCOLLEGE ST AT OSSINGTON AVE (PX 829)\t-79.422705\t43.65439999619167\n",
    "# 241\t4170\tCOLLEGE ST AT SPADINA AVE (PX 279)\t-79.400048\t43.65794800150128\n",
    "\n",
    "# Input\n",
    "# Output\n",
    "\n",
    "boundary <- location %>%\n",
    "  select(location_id, location, lng, lat) %>%\n",
    "  filter(location_id %in% list(5370, 4180, 5864, 4170)) # boundary intersection ID\n",
    "\n",
    "lng_min <- min(boundary$lng) # west most value since it's negative\n",
    "lng_max <- max(boundary$lng) # east most value\n",
    "lat_min <- min(boundary$lat) # south most value\n",
    "lat_max <- max(boundary$lat) # north most value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine Data for model building\n",
    "# what is the expected output of data structure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 1 - Time Series Forecast\n",
    "# additional data processing needed before modelling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 2 - Regression Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Result and Discussion\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
